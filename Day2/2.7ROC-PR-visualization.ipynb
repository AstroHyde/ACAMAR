{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Confusion Matrix, ROC Curve, and Precision-Recall Curve\n",
    "\n",
    "Author: Dr. Elaina A. Hyde\n",
    "\n",
    "---\n",
    "\n",
    "The interactive visualization below lets you see how the confusion matrix, ROC curve, and Precision-recall curve all interact. \n",
    "\n",
    "The model is a logistic regression fit on the cancer vs. healthy data. The raw datapoints are shown along with the prediction curve (right panel). \n",
    "\n",
    "You can change the threshold and see how it affects the confusion matrix, as well as where that threshold is on the corresponding ROC or precision-recall curve (left panel). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**The visualization allows you to change 3 variables:**\n",
    "- **spread**: the dispersion of the data (impacts the signal and how well the classifier can discriminate between the points).\n",
    "- **threshold**: the decision threshold for labeling 1 vs. 0\n",
    "- **cancer %**: the number of datapoints that are cancer vs. healthy. This helps show the effect of class imbalance on classifier performance and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xantheterra/miniconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c28cb5dbe2487989c4bab19e6b3ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=75, description='spread:'), FloatSlider(value=0.5, description='thresholâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import imp\n",
    "plotter = imp.load_source('plotter', 'roc_plotter.py')\n",
    "from plotter import ROCLogisticPlotter\n",
    "\n",
    "roc_plotter = ROCLogisticPlotter()\n",
    "roc_plotter.preconstruct_data()\n",
    "roc_plotter.roc_interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Relevant classification metrics\n",
    "\n",
    "This reference table describes some of the important metrics displayed in the visual below.\n",
    "\n",
    "|   |   |\n",
    "|---|---|\n",
    "|**TPR/RECALL**    | The true positive rate, also known as the **sensitivity** or **recall**. It is the ability of the classifier to correctly identify a class. <br><br>`Recall = True Positives / (True Positives + False Negatives)`<br><br>A recall of 1 indicates that the classifier correctly predicted all observations of the class.  0 means the classifier predicted all observations of the current class incorrectly.|\n",
    "|**FPR** | The false positive rate is the percent of times model predicts 1 when the class is 0. This is the x-axis on the ROC curve.<br><br> `fpr = fp / (tn + fp)`<br><br>|\n",
    "|**PRECISION** | The ability of the classifier to avoid labeling a class as a member of another class. <br><br> `Precision = True Positives / (True Positives + False Positives)`<br><br>_A precision score of 1 indicates that the classifier never mistakenly classified the current class as another class.  precision score of 0 would mean that the classifier misclassified every instance of the current class_ |\n",
    "|**RECALL**    | The ability of the classifier to correctly identify the current class. <br><br>`Recall = True Positives / (True Positives + False Negatives)`<br><br>A recall of 1 indicates that the classifier correctly predicted all observations of the class.  0 means the classifier predicted all observations of the current class incorrectly.|\n",
    "|**AUC** | The area under the curve: this can refer to either the ROC curve or the precision-recall curve. In the case of the ROC curve, an area of 0.50 is the baseline, meaning this is the area under the curve when the classifier would be predicting at chance. An AUC of 1.0 is a perfect model, where the classifier never makes a mistake. <br><br>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.66      0.67        50\n",
      "        1.0       0.67      0.70      0.69        50\n",
      "\n",
      "avg / total       0.68      0.68      0.68       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(roc_plotter.currents['y_true'], roc_plotter.currents['y_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {
    "872e42c263fe4ec8a357b222ada0a21b": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
